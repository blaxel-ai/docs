---

title: 'Deploy a LangChain agent'

description: 'Deploy your first LangChain AI agent on Beamlit.'

---

This tutorial shows how to deploy a custom [LangChain](https://python.langchain.com/docs/introduction/) AI agent directly from your IDE using the Beamlit CLI. Your agent will typically include these key components:

- A **core agent logic algorithm** â†’ Beamlit will deploy this as an [Agent](../Agents/Overview)
- A **set of tools that the agent can use** â†’ Beamlit will deploy these in sandboxed environments as [Functions](../Functions/Overview)
- A **chat model that powers the agent** â†’ Beamlit can request a [Beamlit model API](../Models/Overview) upon LLM calling.

When the main agent runs, Beamlit orchestrates everything in secure, sandboxed environments, providing complete traceability of all requests and production-grade scalability.

## **Prerequisites**

- A Beamlit workspace
- The [Beamlit CLI installed](../cli-reference/introduction) on your machine and [logged in](https://docs.beamlit.com/cli-reference/bl_login) to your workspace

## **Guide**

### (Optional) Quickstart from a template

Letâ€™s initialize a first app. The following command creates a **pre-scaffolded local repository** ready for developing and deploying your agent on Beamlit. You will need to [have *uv* installed](https://docs.astral.sh/uv/getting-started/installation/) for this.

```bash
bl create-agent-app my-agent
```

Select your model API, choose the *Custom* template, and press Enter to create the repo. 

### Add your agent code

Add or import the script files containing your LangChain agent. Here's an example using a simple LangChain React agent:

```python agent.py

from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from customfuncs.helloworld import helloworld

chat = ChatOpenAI()
memory = MemorySaver()
custom_agent = create_react_agent(chat, tools=[helloworld], checkpointer=memory)

```

Note that this code references a custom tool from another folder: `customfuncs.helloworld.helloworld`. By default, this function **will not** be deployed as a separate component. See next section for instructions on deploying it in its own sandboxed environment.

The next step is to use [the Beamlit SDK](../Agents/Develop-an-agent) to specify which function should be deployed on Beamlit. You'll need two key elements:

- Create a **main async function** to handle agent executionâ€”this will be your default entry point for serving and deployment.
- Add the **`@agent` decorator** to designate your main agent function for Beamlit serving.

Here's an example showing the main async function:

```python agent.py {11-31}

from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from customfuncs.helloworld import helloworld

chat = ChatOpenAI()
memory = MemorySaver()
custom_agent = create_react_agent(chat, tools=[helloworld], checkpointer=memory)

async def main(
    agent: Union[None, CompiledGraph],
    chat_model: Union[None, BaseChatModel],
    tools: list[BaseTool],
    request: Request,
    **_,
):
    body = await request.json()
    if len(tools) > 0:
        agent.bind_tools(tools)
    agent_config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    if body.get("inputs"):
        body["input"] = body["inputs"]

    agent_body = {"messages": [("user", body["input"])]}
    responses = []

    async for chunk in agent.astream(agent_body, config=agent_config):
        responses.append(chunk)
    content = responses[-1]
    return content["agent"]["messages"][-1].content

```

Read [our reference for the main agent function to serve](../Agents/Develop-an-agent).

Then, hereâ€™s an example adding the Beamlit decorator:

```python agent.py {1,11}
from beamlit.agents import agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from customfuncs.helloworld import helloworld

chat = ChatOpenAI()
memory = MemorySaver()
custom_agent = create_react_agent(chat, tools=[helloworld], checkpointer=memory)

@agent(override_agent=custom_agent)
async def main(
    agent: Union[None, CompiledGraph],
    chat_model: Union[None, BaseChatModel],
    tools: list[BaseTool],
    request: Request,
    **_,
):
    body = await request.json()
    if len(tools) > 0:
        agent.bind_tools(tools)
    agent_config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    if body.get("inputs"):
        body["input"] = body["inputs"]

    agent_body = {"messages": [("user", body["input"])]}
    responses = []

    async for chunk in agent.astream(agent_body, config=agent_config):
        responses.append(chunk)
    content = responses[-1]
    return content["agent"]["messages"][-1].content

```

Read [our reference for @agent decorator](../Agents/Develop-an-agent).

At this time: 

- your agent is ready to be deployed on Beamlit
- functions (tool calls) and model APIs **are not ready** to be deployed on separate sandboxed environments.

For production deployments of AI agents, it's recommended to use a microservices architecture where each component runs independently in its own environment. **Beamlit helps you cross this last mile with just a few lines of code.**

### Sandbox the tool call execution

To deploy your tool on Beamlit, simply place the main function's file in the `src/functions/` folder.

Hereâ€™s an example with the helloworld custom tool from the previous code snippets:

```python src/functions/helloworld.py

async def helloworld(query: str):
    """
    A useful tool that says hello to the world.
    """
    return "Hello, world!"

```

Now, add the `@function` decorator to specify the default entry point for serving and deployment of the function.

```python src/functions/helloworld.py {1-3}
from beamlit.functions import function

@function()
async def helloworld(query: str):
    """
    A useful tool that says hello to the world.
    """
    return "Hello, world!"

```

Functions placed in the `src/functions/` folder will be deployed as custom functions and automatically become available to the agent during execution. The final step is to remove the hardcoded tool binding from the main agent file:

```python agent.py {8}
from beamlit.agents import agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

chat = ChatOpenAI()
memory = MemorySaver()
custom_agent = create_react_agent(chat, tools=[], checkpointer=memory)

@agent(override_agent=custom_agent)
async def main(
    agent: Union[None, CompiledGraph],
    chat_model: Union[None, BaseChatModel],
    tools: list[BaseTool],
    request: Request,
    **_,
):
    body = await request.json()
    if len(tools) > 0:
        agent.bind_tools(tools)
    agent_config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    if body.get("inputs"):
        body["input"] = body["inputs"]

    agent_body = {"messages": [("user", body["input"])]}
    responses = []

    async for chunk in agent.astream(agent_body, config=agent_config):
        responses.append(chunk)
    content = responses[-1]
    return content["agent"]["messages"][-1].content

```

### Sandbox the model API call

Use the [Beamlit console](https://app.beamlit.com) to create the corresponding **integration connection** to your LLM provider, using one of the supported [integrations](../Integrations/OpenAI).

Then create a [model API](../Models/Overview) using your integration, and **deploy** it. Hereâ€™s an example from the Beamlit console using an OpenAI model:

![Screenshot 2024-12-13 at 4.03.53â€¯PM.png](../Get-started/Screenshot_2024-12-13_at_4.03.53_PM.png)

Finally, configure your agent to use the model API in the code of your main agent file:

```python agent.py {8}
from beamlit.agents import agent
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

memory = MemorySaver()
custom_agent = create_react_agent(chat, tools=[], checkpointer=memory)

@agent(agent={"spec": {"model": "gpt-4o-mini",},}, override_agent=custom_agent)
async def main(
    agent: Union[None, CompiledGraph],
    chat_model: Union[None, BaseChatModel],
    tools: list[BaseTool],
    request: Request,
    **_,
):
    body = await request.json()
    if len(tools) > 0:
        agent.bind_tools(tools)
    agent_config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    if body.get("inputs"):
        body["input"] = body["inputs"]

    agent_body = {"messages": [("user", body["input"])]}
    responses = []

    async for chunk in agent.astream(agent_body, config=agent_config):
        responses.append(chunk)
    content = responses[-1]
    return content["agent"]["messages"][-1].content

```

At this time: your agent, functions and model API are ready to be deployed on Beamlit in sandboxed environments.

### Test and deploy your AI agent

Run the following command to serve your agent locally:

```bash
cd my-agent && source .venv/bin/activate;
bl serve --hotreload;
```

Query your agent locally by making a **POST** request to this endpoint: [`http://localhost:1338`](http://localhost:1338) with the following payload format: `{"inputs": "Hello world!"}`.

To push to Beamlit on the default *production* environment, run the following command. Youâ€™ll need to [have *Docker* installed](https://docs.docker.com/get-started/get-docker/) and running on your machine to build the app.

```bash
bl deploy
```

Thatâ€™s it! ğŸŒÂ ğŸŒÂ ğŸŒÂ Your agent is now **distributed and available** across the entire Beamlit global infrastructure!Â [Global Inference Network](../Models/Global-Inference-Network) significantly speeds up inferences by executing your agentâ€™s workloads in sandboxed environments and smartly routing requests based on your policies.

### Make a first inference

Run a first inference on your Beamlit agent with the following command:

```bash
bl run agent my-agent --data '{"inputs":"Hello world!"}'
```

<Tip>Your agent is available behind a **global endpoint**. Read [this guide](../Agents/Query-agents) on how to use it for HTTP requests.</Tip>

You can also run inference requests on your agent (or each function or model API) from the Beamlit console by using the Playground on the console.

## Next steps

You are ready to run AI everywhere with the Beamlit platform! Check out the following guides which may be useful to you:

<Card title="Deploy agents" icon="earth-americas" href="/Agents/Overview">
Complete guide for deploying AI agents on Beamlit.
</Card>

<Card title="Manage environment policies" icon="server" href="/Model-Governance/Overview">
Complete guide for managing deployment and routing policies on the Global Inference Network.
</Card>

<Card title="Guide for querying agents" icon="bolt" href="/Agents/Query-agents">
Complete guide for querying your AI agents on the Global Inference Network.
</Card>