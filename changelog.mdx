---

title: 'Changelog'

description: 'Keep track of changes and updates on Blaxel.'

---

<Update label="2025-06-06">

### Sandbox tools for codegen

- Added fast apply of file edits powered by [MorphLLM](https://morphllm.com/) for efficient code changes (2000+ tokens/second)
- Added multiple other codegen-optimized functions by making tool calls through the MCP server of a sandbox.

</Update>

<Update label="2025-06-01">

### Deploy from Dockerfile

- Added possibility to deploy agent, MCP server and job from a Dockerfile

</Update>

<Update label="2025-05-27">

### New MCP server: Airweave

<Frame>
<img src="/img/2025/airweave.png" />
</Frame>

- Added new MCP server: Airweave

</Update>

<Update label="2025-05-26">

### Improved latency on sandbox

- Reduced typical duration of sandbox creation down to 4-8 seconds
- Reduced latency on calls made to a sandbox

</Update>

<Update label="2025-05-23">

### New MCP server: Supabase

<Frame>
<img src="/img/2025/supabase.png" />
</Frame>

- Added new MCP server: Supabase

</Update>

<Update label="2025-05-22">

### New Logs console

<Frame>
<img src="/img/2025/logs.png" />
</Frame>

- Reworked interface of Logs console
- Added ability to filter log queries

</Update>

<Update label="2025-05-19">

### Sandboxes file system improvements

- Added ability to [watch sub-directories](https://docs.blaxel.ai/Sandboxes/Filesystem#watch-sub-directories)
- Added ability to [ignore](https://docs.blaxel.ai/Sandboxes/Filesystem#ignore-files-or-directories) some files/directories from watch
- Added ability to [write multiple files](https://docs.blaxel.ai/Sandboxes/Filesystem#write-multiple-files) at once
- Added ability to [write binary content](https://docs.blaxel.ai/Sandboxes/Filesystem#write-binary) in the fs
- Added ability to get both stderr/stdout logs at once in batch mode

</Update>

<Update label="2025-05-15">

### OneGrep Integration

<Frame>
<img src="/img/2025/onegrep.png" />
</Frame>

- Added an agent template that uses [OneGrep](https://www.onegrep.dev/) for semantic tool searching and selection

</Update>

<Update label="2025-05-14">

### Sessions for sandboxes

- Added ability to create sessions to operate sandboxes from a frontend client

</Update>

<Update label="2025-05-09">

### New MCP servers: HubSpot & Smartlead

- Added new MCP server: HubSpot
- Added new MCP server: Smartlead

</Update>

<Update label="2025-05-06">

### Preview URLs for sandboxes

- Added support for public preview URLs for sandboxes

</Update>

<Update label="2025-05-03">

### Async endpoint for agents

- Added endpoint to send asynchronous requests to an agent

</Update>

<Update label="2025-05-01">

### Sandboxes

<Frame>
<img src="/img/2025/sbx-cold-start.png" />
</Frame>

Our biggest release since we launched: Sandboxes, running on our new Mark 3 Infrastructure!

Sandboxes are fast-launching virtual machines built for agentic AIs. Agents can use them to run commands or render code in an isolated environment.

This protects your infrastructure when running AI-generated code, enabling use cases such as codegen, AI code review, AI data analysis and more.

They are powered by our brand-new Mark 3 infrastructure generation that pushes the limits of infrastructure performance. Key features of Mk 3 sandboxes:

- Boot time under 20ms
- Persistent filesystem across sessions
- Operable via both [Python/TypeScript SDK](Sandboxes/Overview) and MCP server

Sandboxes and Mk 3 infrastructure are currently in private Alpha release. [Contact us](http://blaxel.ai/contact?purpose=sandbox) to get access.

</Update>

<Update label="2025-04-25">

### New MCP: Tavily

<Frame>
<img src="/img/2025/tavily.png" />
</Frame>

- Added new MCP server: Tavily. Enable your agents to search the web with Tavily’s API.

</Update>

<Update label="2025-04-17">

### New Blaxel Console

<Frame>
<img src="/img/2025/newui.png" />
</Frame>

- We reworked the UI of Blaxel Console! Now more geeky with loads of new metrics and visibility on your infrastructure — and an integrated changelog.

</Update>

<Update label="2025-04-12">

### New framework supported: Google ADK

<Frame>
<img src="/img/2025/adk.png" />
</Frame>

- Added support for Google ADK (Agent Development Kit) framework

</Update>

<Update label="2025-04-07">

### New framework supported: PydanticAI

- Added support for PydanticAI framework

</Update>

<Update label="2025-04-02">

### SDK v0.1.0

<Frame>
<img src="/img/2025/sdk010.png" />
</Frame>

- Released a new [major version of our SDK](Agents/Develop-an-agent) to give you access to lower-level features:
    - host custom MCP servers
    - connect to LLM via Blaxel gateway
    - connect to tools hosted on Blaxel
- Compatibility with all major agentic frameworks from day 1: LangChain, LangGraph, CrewAI, LlamaIndex, OpenAI Agents, Vercel AI SDK, Mastra.
- Improved cold-starts for agents and functions

</Update>

<Update label="2025-03-20">

### Functions are now MCP servers

<Frame>
<img src="/img/2025/mcp-servers-functions.png" />
</Frame>

- All functions on Blaxel are now exposed as MCP servers, even custom functions.
- Create a custom MCP server from a scaffolded repo using `bl create-mcp-server`

</Update>

<Update label="2025-03-06">

### Templates of agents

<Frame>
<img src="/img/2025/templates-ui.png" />
</Frame>

- Added 6 templates of agents
- Templates can be used when creating a new agent from the Blaxel Console. Deploying a template will deploy the agent on your GitHub organization and setup a live synchronization watching future updates
- The entire template list is available on [our website](https://blaxel.ai/templates)
- Added information in Blaxel Console that an agent is synchronized via GitHub
- Updated the schema for functions to better match MCP standard

</Update>

<Update label="2025-02-28">

### Improved build times

Functions and agents now build and deploy 100% to 200% faster than before, thanks to a reworked build system. 

</Update>

<Update label="2025-02-24">

### Voice agents

We've added a new low-latency voice agent template that enables real-time speech interaction with AI systems. Built using OpenAI's Realtime API and LangGraph ReAct agent, this template supports multi-modal inputs/outputs and tool calling via Blaxel Functions.

</Update>

<Update label="2025-02-18">

### Revisions & canary deployments

<Frame>
<img src="/img/2025/revisions.png" />
</Frame>

Added revisions for agents, model APIs and functions:

- Each build of an object creates a new immutable revision, which can be deployed to redirect request traffic to it
- Ability to deploy a new revision using blue-green strategy
- Ability to rollback to previous revision
- Ability to split a percentage of traffic to a second revision (canary deployments)

</Update>

<Update label="2025-02-17">

### Beamlit is now Blaxel!

<Frame>
<img src="/img/2025/blaxel.png" />
</Frame>

We're excited to announce our official name change from Beamlit to Blaxel! 

</Update>

<Update label="2025-02-14">

### Sunsetting environments

We are discontinuing Beamlit environments. Stay tuned for an upcoming major update that will help you better manage your deployment lifecycles.

</Update>

<Update label="2025-02-12">

### New pricing plans

<Frame>
<img src="/img/2025/pricing.png" />
</Frame>

Beamlit officially launched new pricing plans. Learn more about the pricing plans on [our website](https://blaxel.ai/), or login to the [Beamlit console](https://app.blaxel.ai/main/global-agentic-network) to start your free tier or upgrade.

</Update>

<Update label="2025-02-07">

### DeepSeek integration

<Frame>
<img src="/img/2025/deepseek-integration.png" />
</Frame>

- Connect to DeepSeek models from Beamlit

</Update>

<Update label="2025-02-04">

### HuggingFace integration

<Frame>
<img src="/img/2025/hf-integration.png" />
</Frame>

- Connect to public or private models from HuggingFace [Inference API (serverless)](https://huggingface.co/docs/api-inference/index) and [Inference Endpoints (dedicated)](https://huggingface.co/docs/inference-endpoints/index)
- Deploy a model in HuggingFace [Inference Endpoints (dedicated)](https://huggingface.co/docs/inference-endpoints/index)

</Update>

<Update label="2025-01-31">

### Azure AI Foundry integration

<Frame>
<img src="/img/2025/azure-ai-foundry.png" />
</Frame>

- New [integration](Integrations/Azure-AI-Foundry): **Azure AI Services** (for OpenAI models and others)
- New [integration](Integrations/Azure-AI-Foundry): **Azure Marketplace** (for Llama models and others)

</Update>

<Update label="2025-01-30">

### 9 new pre-built toolkits

<Frame>
<img src="/img/2025/mcp-functions.png" />
</Frame>

Added 9 new prebuilt templates of [functions](Functions/Overview):

- Brave Search
- Google Maps
- Slack
- Linear
- AWS SES
- Cloudflare
- PostgreSQL
- AWS S3
- Dall-E

</Update>

<Update label="2025-01-28">

### Policies on token usage for cost control

<Frame>
<img src="/img/2025/chart-token.png" />
</Frame>

- New [policy](Model-Governance/Policies) type: maximum token usage

</Update>

<Update label="2025-01-26">

### 🚀 We launched Beamlit beta!

<Frame>
<img src="/img/2025/overview-with-map.png"/>
</Frame>

Beamlit Beta is available publicly! Coming with the following new features:

- New world-class infrastructure for minimal cold starts and maximum reliability
- World map showing the origin of requests on your agents AI
- Many additional charts

</Update>

<Update label="2025-01-20">

### Agentic traces

<Frame>
<img src="/img/2025/trace-jan29.png" />
</Frame>

- Added Agent Observability suite, with traces of agents’ requests.
- Added ability to launch a request in *debug* mode from the Playground to save the trace

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2025-01-12">

### Latency metric tracking

<Frame>
<img src="/img/2025/chart-latencies.png" />
</Frame>

- Added a new set of metrics is available for your agents, functions and models: **end-to-end latencies (average, p50, p90, p99)**.

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2025-01-07">

### Beamlit extension for VScode

<Frame>
<img src="/img/2025/vscodeextension.png" />
</Frame>

[Click here to download](https://marketplace.visualstudio.com/items?itemName=Beamlit.vscode-beamlit-tools) the Beamlit extension for Visual Studio Code.

- View and retrieve your workspace’s resources directly from the VSCode IDE

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2025-01-03">

### New Beamlit console

<Frame>
<img src="/img/2025/newconsole.png" />
</Frame>

- Beamlit console has been completely reworked, in order to center the experience around agents and their deployments.
- Removed custom model deployments and renamed external models “model APIs”
- Added statuses for deployments
- Added playbook to deploy an agent from local code in the Beamlit console

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-20">

### Visual aspect of charts is improved

<Frame>
<img src="/img/2024-12-20/metric.png" />
</Frame>

- Improved visual aspect of charts and options to change time window of screens
- API reference for agents’, models’ and functions’ deployments has been updated

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-17">

### CLI support for running agents and functions

- You can now use the Beamlit CLI to run requests on agents (`bl run agent`) and functions (`bl run function`)

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-16" description="Total requests metric">

- Added new metric: total number of requests

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-11" description="Develop agents locally with the Beamlit SDK">

- You can now develop (and run) your own AI agents **locally** with the Beamlit SDK.
- Use CLI command `bl create-agent-app your-agent-name` to initialize a repository with the code skeleton to get started
- Use CLI command `bl serve --local` to run your AI agent on your local machine

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-03" description="New integrations">
2 new **integrations** are available to connect to external AI model providers:

- Cohere
- xAI

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-12-02" description="GitHub Action for Beamlit">
You can now use a **GitHub Action** to directly deploy Beamlit objects (agents, functions, models) from your GitHub Workflows.

- [https://github.com/beamlit/bl-action](https://github.com/beamlit/bl-action)

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-11-28" description="External AI model providers">
Happy Thanksgiving! You can now connect **external model providers** as integrations in your Beamlit workspace in order to leverage Beamlit’s accelerated Global Inference Network to unify calls to third-party APIs.

- OpenAI
- Anthropic
- Mistral AI

Build and run agents globally while centralizing model access, credentials and observability behind our production-ready global gateway. 🌐

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>

<Update label="2024-11-21" description="Beamlit’s private alpha launch">
Experience Beamlit first-hand and discover what it can you for you. Join the [private alpha waitlist for Beamlit now](https://beamlit.com/beta-signup).

### Features

- **Agents**: run AI agents across multiple locations, so your consumers get the lowest latency and the highest availability.
- **Models**: connect or deploy generative AI models behind our global gateway, from public endpoints to custom fine-tuned models
- **Functions**: run serverless functions that provide your agents with the tools to interact with their environment
- **Global Inference Network**: make your agents available globally and locally with a network designed for AI inferences that optimizes for mission-critical latency while respecting your deployment, routing and cost policies.
- **Environments**: manage compliance at enterprise level by enforcing policies directly in your development life-cycle
- **Policies**: define global rules and strategies regarding your deployment placement, inference request routing, and hardware usage.

Beamlit is currently in private alpha. Join our [waitlist for access today](https://beamlit.com/beta-signup).

</Update>